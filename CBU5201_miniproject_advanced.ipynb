{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16834fe2-1f5f-4267-aa69-1df79edf4348",
   "metadata": {},
   "source": [
    "1 Author\n",
    "Student Name:Rui Chen\n",
    "Student ID:210978863"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11879fda-2790-4430-9836-410e28dc2afb",
   "metadata": {},
   "source": [
    "2 Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee0fb4-d4d5-45b4-a968-f7e9f2cabe51",
   "metadata": {},
   "source": [
    "In this task,I plan to do a multi classification based on genki4k dataset to classify these people with their gender and color and get six catagories.Because I found that this dataset covers all races and genders, and by labelling it, I was able to get the machine to learn its features then do multi-classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b834e1-c0c1-4a70-9ef9-c5543ac35330",
   "metadata": {},
   "source": [
    "3 Machine Learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67803cf2-a261-4d43-8fb6-b1603ebdc537",
   "metadata": {},
   "source": [
    "There are four main step to do the task:data preprocess,split data,train the model,test(evaluate) the model.\n",
    "\n",
    "In this task, there are also other algorithm like SVM, decision tree and so on to do multi classification, but I still use resnet34 to do, most of steps are same as basic part, we just need to adjust our datasets and make some changes of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460eb69-e1ed-46d6-ae1b-830fac3fbe62",
   "metadata": {},
   "source": [
    "4 Transformation Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564045c8-d46e-41ca-952a-2665de94ffd3",
   "metadata": {},
   "source": [
    "Same as basic part, we use 68_face_landmarks to extract human face, it can be seen like a mild data augmentation because we extract useful information to fit a model. I do a simple geometric transformation and resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b3b74-31fa-4803-b67c-a3816d9ff749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show(\"genki4k/files/file0001.jpg\")\n",
    "plt.show(\"genki4k/files(preprocess)/file0001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51340507-5b6f-4a41-a5a0-74cda09b1824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# File paths for images\n",
    "image_path1 = 'genki4k/files/file0001.jpg'\n",
    "image_path2 = 'genki4k/files(preprocess)/file1.jpg'\n",
    "\n",
    "# Display the two images\n",
    "display(Image(filename=image_path1))\n",
    "display(Image(filename=image_path2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a466b8-2875-4caa-bf32-45f6f1f658e2",
   "metadata": {},
   "source": [
    "5 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3eb38a-8e51-4b84-b016-b76943136cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from resnet import resnet34\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load pretrain weights\n",
    "    # download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "    model_weight_path = \"./resnet34-pre.pth\"\n",
    "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
    "\n",
    "    net = resnet34()\n",
    "    net.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "    # change fc layer structure\n",
    "    in_channel = net.fc.in_features\n",
    "    net.fc = nn.Linear(in_channel, 5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef50f03-8152-4525-bb38-0684c941d7c2",
   "metadata": {},
   "source": [
    "In multi classification task, we use a another package \"resNet34_advanced\" in my folder which is a pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff3560-6366-45a9-987d-945460bf9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "     Note: In the original paper, on the main branch of the dashed residual structure, the first 1x1 convolutional layer has a step of 2 and the second 3x3 convolutional layer has a step of 1.\n",
    "    However, in the official pytorch implementation, the first 1x1 convolutional layer has a step of 1, and the second 3x3 convolutional layer has a step of 2.\n",
    "    The advantage of doing this is that it can improve the accuracy of top1 by about 0.5%.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None,\n",
    "                 groups=1, width_per_group=64):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        width = int(out_channel * (width_per_group / 64.)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,\n",
    "                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,\n",
    "                               kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,\n",
    "                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 blocks_num,\n",
    "                 num_classes=1000,\n",
    "                 include_top=True,\n",
    "                 groups=1,\n",
    "                 width_per_group=64):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.in_channel = 64\n",
    "\n",
    "        self.groups = groups\n",
    "        self.width_per_group = width_per_group\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n",
    "        if self.include_top:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride,\n",
    "                            groups=self.groups,\n",
    "                            width_per_group=self.width_per_group))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel,\n",
    "                                channel,\n",
    "                                groups=self.groups,\n",
    "                                width_per_group=self.width_per_group))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.include_top:\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet34(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet50(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet101(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\n",
    "    groups = 32\n",
    "    width_per_group = 4\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3],\n",
    "                  num_classes=num_classes,\n",
    "                  include_top=include_top,\n",
    "                  groups=groups,\n",
    "                  width_per_group=width_per_group)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\n",
    "    groups = 32\n",
    "    width_per_group = 8\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3],\n",
    "                  num_classes=num_classes,\n",
    "                  include_top=include_top,\n",
    "                  groups=groups,\n",
    "                  width_per_group=width_per_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb313e-2e62-458b-af8a-a7bc4b3dc5df",
   "metadata": {},
   "source": [
    "6 Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d9f81-74f8-473d-acc4-f5458f73c962",
   "metadata": {},
   "source": [
    "Step 1 preprocess (This is explained in detail in Part 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c27fb-e1a6-4544-82fe-1807be23a890",
   "metadata": {},
   "source": [
    "Step 2 split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eddfd0-3998-4e79-91cb-76af9f0f3230",
   "metadata": {},
   "source": [
    "I split the dataset according to my own labels, the fifth means gender,\"0\" is woman and \"1\" means man; the sixth value in a line means color,\"1\" means yellow, \"2\" means white and \"3\" means black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c42420-45a4-421c-b6e2-cfd5633a3f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split the dataset done!\n",
      "split into train,test or val done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_into_two(i=None):\n",
    "    \n",
    "    data_folder = \"genki4k/files(preprocess)\"\n",
    "    label_file= \"genki4k/labels.txt\"\n",
    "\n",
    "    # create output folders\n",
    "    os.makedirs(\"genki4k/files(splitdata)/woman_y\", exist_ok=True)\n",
    "    os.makedirs(\"genki4k/files(splitdata)/woman_w\", exist_ok=True)\n",
    "    os.makedirs(\"genki4k/files(splitdata)/woman_b\", exist_ok=True)\n",
    "    os.makedirs(\"genki4k/files(splitdata)/man_y\", exist_ok=True)\n",
    "    os.makedirs(\"genki4k/files(splitdata)/man_w\", exist_ok=True)\n",
    "    os.makedirs(\"genki4k/files(splitdata)/man_b\", exist_ok=True)\n",
    "    # get all the files\n",
    "    files = os.listdir(data_folder)\n",
    "\n",
    "    # read label.txt\n",
    "    with open(label_file, 'r') as label_file:\n",
    "        lines = label_file.readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # split each line\n",
    "        values = line.strip().split()\n",
    "        fnames = ['file{}.jpg'.format(i + 1)]\n",
    "\n",
    "        # make sure at least six value\n",
    "        if len(values) >= 6:\n",
    "            fifth_value = values[4]\n",
    "            sixth_value = values[5]\n",
    "        \n",
    "            # When the fifth number is 0 and sixth is 1ï¼Œcopy to the woman_y\n",
    "            if (fifth_value == '0')& (sixth_value == '1'):\n",
    "                 for fname in fnames:\n",
    "                      src = os.path.join(\"genki4k/files(preprocess)\", fname)\n",
    "                      dst = os.path.join(\"genki4k/files(splitdata)/woman_y\", fname)\n",
    "                      shutil.copyfile(src, dst)\n",
    "            if (fifth_value == '0')& (sixth_value == '2'):\n",
    "                 for fname in fnames:\n",
    "                      src = os.path.join(\"genki4k/files(preprocess)\", fname)\n",
    "                      dst = os.path.join(\"genki4k/files(splitdata)/woman_w\", fname)\n",
    "                      shutil.copyfile(src, dst)\n",
    "            if (fifth_value == '0')& (sixth_value == '3'):\n",
    "                 for fname in fnames:\n",
    "                      src = os.path.join(\"genki4k/files(preprocess)\", fname)\n",
    "                      dst = os.path.join(\"genki4k/files(splitdata)/woman_b\", fname)\n",
    "                      shutil.copyfile(src, dst)\n",
    "            if (fifth_value == '1')& (sixth_value == '1'):\n",
    "                 for fname in fnames:\n",
    "                      src = os.path.join(\"genki4k/files(preprocess)\", fname)\n",
    "                      dst = os.path.join(\"genki4k/files(splitdata)/man_y\", fname)\n",
    "                      shutil.copyfile(src, dst)\n",
    "            if (fifth_value == '1')& (sixth_value == '2'):\n",
    "                 for fname in fnames:\n",
    "                      src = os.path.join(\"genki4k/files(preprocess)\", fname)\n",
    "                      dst = os.path.join(\"genki4k/files(splitdata)/man_w\", fname)\n",
    "                      shutil.copyfile(src, dst)\n",
    "            if (fifth_value == '1')& (sixth_value == '3'):\n",
    "                 for fname in fnames:\n",
    "                      src = os.path.join(\"genki4k/files(preprocess)\", fname)\n",
    "                      dst = os.path.join(\"genki4k/files(splitdata)/man_b\", fname)\n",
    "                      shutil.copyfile(src, dst)\n",
    "\n",
    "    print(\"split the dataset done!\")\n",
    "\n",
    "\n",
    "def split_into_three():\n",
    "    split_ratio = [0.6, 0.2, 0.2]\n",
    "\n",
    "    output_folder = [\"genki4k/files(splitdata)/train\", \"genki4k/files(splitdata)/test\",\n",
    "                     \"genki4k/files(splitdata)/val\"]\n",
    "    for folder in output_folder:\n",
    "        os.makedirs(os.path.join(folder, \"man_b\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(folder, \"man_w\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(folder, \"man_y\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(folder, \"woman_w\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(folder, \"woman_b\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(folder, \"woman_y\"), exist_ok=True)\n",
    "\n",
    "    # split man_b\n",
    "    man_b_files = os.listdir(\"genki4k/files(splitdata)/man_b\")\n",
    "    random.shuffle(man_b_files)\n",
    "    total_man_b_files = len(man_b_files)\n",
    "    split_index_man_b = [int(total_man_b_files * split_ratio[0]),\n",
    "                         int(total_man_b_files * (split_ratio[0] + split_ratio[1]))]\n",
    "\n",
    "    for i, file in enumerate(man_b_files):\n",
    "        if i < split_index_man_b[0]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_b\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/train\", \"man_b\", file))\n",
    "        elif i < split_index_man_b[1]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_b\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/test\", \"man_b\", file))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_b\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/val\", \"man_b\", file))\n",
    "\n",
    "    man_w_files = os.listdir(\"genki4k/files(splitdata)/man_w\")\n",
    "    random.shuffle(man_w_files)\n",
    "    total_man_w_files = len(man_w_files)\n",
    "    split_index_man_w = [int(total_man_w_files * split_ratio[0]),\n",
    "                         int(total_man_w_files * (split_ratio[0] + split_ratio[1]))]\n",
    "\n",
    "    for i, file in enumerate(man_w_files):\n",
    "        if i < split_index_man_w[0]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_w\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/train\", \"man_w\", file))\n",
    "        elif i < split_index_man_w[1]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_w\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/test\", \"man_w\", file))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_w\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/val\", \"man_w\", file))\n",
    "\n",
    "    man_y_files = os.listdir(\"genki4k/files(splitdata)/man_y\")\n",
    "    random.shuffle(man_y_files)\n",
    "    total_man_y_files = len(man_y_files)\n",
    "    split_index_man_y = [int(total_man_y_files * split_ratio[0]),\n",
    "                         int(total_man_y_files * (split_ratio[0] + split_ratio[1]))]\n",
    "\n",
    "    for i, file in enumerate(man_y_files):\n",
    "        if i < split_index_man_y[0]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_y\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/train\", \"man_y\", file))\n",
    "        elif i < split_index_man_y[1]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_y\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/test\", \"man_y\", file))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/man_y\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/val\", \"man_y\", file))\n",
    "\n",
    "    # split woman_b folder\n",
    "    woman_b_files = os.listdir(\"genki4k/files(splitdata)/woman_b\")\n",
    "    random.shuffle(woman_b_files)\n",
    "    total_woman_b_files = len(woman_b_files)\n",
    "    split_index_woman_b = [int(total_woman_b_files * split_ratio[0]),\n",
    "                           int(total_woman_b_files * (split_ratio[0] + split_ratio[1]))]\n",
    "\n",
    "    for i, file in enumerate(woman_b_files):\n",
    "        if i < split_index_woman_b[0]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_b\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/train\", \"woman_b\", file))\n",
    "        elif i < split_index_woman_b[1]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_b\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/test\", \"woman_b\", file))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_b\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/val\", \"woman_b\", file))\n",
    "\n",
    "    woman_w_files = os.listdir(\"genki4k/files(splitdata)/woman_w\")\n",
    "    random.shuffle(woman_w_files)\n",
    "    total_woman_w_files = len(woman_w_files)\n",
    "    split_index_woman_w = [int(total_woman_w_files * split_ratio[0]),\n",
    "                           int(total_woman_w_files * (split_ratio[0] + split_ratio[1]))]\n",
    "\n",
    "    for i, file in enumerate(woman_w_files):\n",
    "        if i < split_index_woman_w[0]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_w\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/train\", \"woman_w\", file))\n",
    "        elif i < split_index_woman_w[1]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_w\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/test\", \"woman_w\", file))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_w\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/val\", \"woman_w\", file))\n",
    "\n",
    "    woman_y_files = os.listdir(\"genki4k/files(splitdata)/woman_y\")\n",
    "    random.shuffle(woman_y_files)\n",
    "    total_woman_y_files = len(woman_y_files)\n",
    "    split_index_woman_y = [int(total_woman_y_files * split_ratio[0]),\n",
    "                           int(total_woman_y_files * (split_ratio[0] + split_ratio[1]))]\n",
    "\n",
    "    for i, file in enumerate(woman_y_files):\n",
    "        if i < split_index_woman_y[0]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_y\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/train\", \"woman_y\", file))\n",
    "        elif i < split_index_woman_y[1]:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_y\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/test\", \"woman_y\", file))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(\"genki4k/files(splitdata)/woman_y\", file),\n",
    "                        os.path.join(\"genki4k/files(splitdata)/val\", \"woman_y\", file))\n",
    "\n",
    "    print(\"split into train,test or val done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_into_two()\n",
    "    split_into_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7412c7-11bc-4e8f-bb1b-819f47be7d41",
   "metadata": {},
   "source": [
    "Step 3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9811228b-c067-44ae-a4ec-bb51efc125c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu device.\n",
      "Using 8 dataloader workers every process\n",
      "using 476 images for training, 162 images for validation.\n",
      "train epoch[1/10] loss:0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:15<00:00,  2.50s/it]\n",
      "valid epoch[1/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:11<00:00,  1.03s/it]\n",
      "[epoch 1] train_loss: 1.418  val_accuracy: 0.549\n",
      "train epoch[2/10] loss:0.856: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:44<00:00,  3.47s/it]\n",
      "valid epoch[2/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:15<00:00,  1.44s/it]\n",
      "[epoch 2] train_loss: 1.061  val_accuracy: 0.543\n",
      "train epoch[3/10] loss:1.091: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:47<00:00,  3.58s/it]\n",
      "valid epoch[3/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:16<00:00,  1.47s/it]\n",
      "[epoch 3] train_loss: 0.880  val_accuracy: 0.667\n",
      "train epoch[4/10] loss:0.834: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:13<00:00,  2.47s/it]\n",
      "valid epoch[4/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.04it/s]\n",
      "[epoch 4] train_loss: 0.784  val_accuracy: 0.648\n",
      "train epoch[5/10] loss:0.797: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:10<00:00,  2.36s/it]\n",
      "valid epoch[5/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.06it/s]\n",
      "[epoch 5] train_loss: 0.703  val_accuracy: 0.642\n",
      "train epoch[6/10] loss:0.792: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:10<00:00,  2.36s/it]\n",
      "valid epoch[6/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.06it/s]\n",
      "[epoch 6] train_loss: 0.631  val_accuracy: 0.660\n",
      "train epoch[7/10] loss:0.194: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:10<00:00,  2.34s/it]\n",
      "valid epoch[7/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.07it/s]\n",
      "[epoch 7] train_loss: 0.526  val_accuracy: 0.636\n",
      "train epoch[8/10] loss:1.200: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:13<00:00,  2.43s/it]\n",
      "valid epoch[8/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.02it/s]\n",
      "[epoch 8] train_loss: 0.560  val_accuracy: 0.673\n",
      "train epoch[9/10] loss:0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:14<00:00,  2.47s/it]\n",
      "valid epoch[9/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.01it/s]\n",
      "[epoch 9] train_loss: 0.556  val_accuracy: 0.642\n",
      "train epoch[10/10] loss:0.768: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:13<00:00,  2.44s/it]\n",
      "valid epoch[10/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:10<00:00,  1.04it/s]\n",
      "[epoch 10] train_loss: 0.555  val_accuracy: 0.722\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from resnet import resnet34\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        \"val\": transforms.Compose([transforms.Resize(256),\n",
    "                                   transforms.CenterCrop(224),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
    "\n",
    "    data_root = os.path.abspath(os.path.join(os.getcwd(), \"../Advanced\"))  # get data root path\n",
    "    image_path = os.path.join(data_root, \"genki4k\", \"files(splitdata)\")  # genki4k data set path\n",
    "    assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"),\n",
    "                                         transform=data_transform[\"train\"])\n",
    "    train_num = len(train_dataset)\n",
    "\n",
    "    flower_list = train_dataset.class_to_idx\n",
    "    cla_dict = dict((val, key) for key, val in flower_list.items())\n",
    "    # write dict into json file\n",
    "    json_str = json.dumps(cla_dict, indent=4)\n",
    "    with open('class_indices_advanced.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    batch_size = 16\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=nw)\n",
    "\n",
    "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"),\n",
    "                                            transform=data_transform[\"val\"])\n",
    "    val_num = len(validate_dataset)\n",
    "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=nw)\n",
    "\n",
    "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                           val_num))\n",
    "    \n",
    "    net = resnet34()\n",
    "    # load pretrain weights\n",
    "    # download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "    model_weight_path = \"./resnet34-pre.pth\"\n",
    "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
    "    net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))\n",
    "    # for param in net.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    # change fc layer structure\n",
    "    in_channel = net.fc.in_features\n",
    "    net.fc = nn.Linear(in_channel, 6)\n",
    "    net.to(device)\n",
    "\n",
    "    # define loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params, lr=0.0001)\n",
    "\n",
    "    epochs = 10\n",
    "    best_acc = 0.0\n",
    "    save_path = './resNet34_advanced.pth'\n",
    "    train_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
    "        for step, data in enumerate(train_bar):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            logits = net(images.to(device))\n",
    "            loss = loss_function(logits, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                     epochs,\n",
    "                                                                     loss)\n",
    "\n",
    "        # validate\n",
    "        net.eval()\n",
    "        acc = 0.0  # accumulate accurate number / epoch\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(validate_loader, file=sys.stdout)\n",
    "            for val_data in val_bar:\n",
    "                val_images, val_labels = val_data\n",
    "                outputs = net(val_images.to(device))\n",
    "                # loss = loss_function(outputs, test_labels)\n",
    "                predict_y = torch.max(outputs, dim=1)[1]\n",
    "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
    "\n",
    "                val_bar.desc = \"valid epoch[{}/{}]\".format(epoch + 1,\n",
    "                                                           epochs)\n",
    "\n",
    "        val_accurate = acc / val_num\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "        if val_accurate > best_acc:\n",
    "            best_acc = val_accurate\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5ed67-d131-4fa9-9595-dfc9e2638f32",
   "metadata": {},
   "source": [
    "Step 4 Test (The detailed process is on part 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334790b4-e9a0-4e18-b72c-4885d22a4791",
   "metadata": {},
   "source": [
    "**7 Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5212da-962c-4e22-b624-5f67b7b23d87",
   "metadata": {},
   "source": [
    "In this task, at first, I labelled each image myself according to the pre-processed images. In the file \"labels.txt\", I typed two labels in the fifth and sixth positions of each line, the first label represents gender, with \"0\" for women and \"0\" for men. The second label is for ethnicity, yellow is \"1\", white is \"2\", black is \"3 The second label is ethnicity, with \"1\" for yellow, \"2\" for white, and \"3\" for black. With the labels, supervised learning is possible.\n",
    "But after I typed in eight hundred labels and tried to classify them, I realised that the dataset was not ethnically homogeneous, especially when you add in the gender; out of the eight hundred graphs, there are two-thirds as many white men as white women, and only one-third as many yellow men as there are white men. At the same time, we do not have a large amount of data, so it will have a large impact on the accuracy, in order to reduce the error caused by this kind of dataset, I manually selected and categorised a part of the photos from the dataset in an attempt to make the dataset a little bit more evenly distributed.\n",
    "As for the pre-processing of each image, it is the same as that of task1, i.e. face extraction based on 68_face_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc451bb-424a-4b26-ae2b-37df433b7404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing done!\n"
     ]
    }
   ],
   "source": [
    "import dlib          \n",
    "import numpy as np   \n",
    "import cv2           \n",
    "import os\n",
    " \n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    " \n",
    "path_read = \"genki4k/files\"\n",
    "num=0\n",
    "for file_name in os.listdir(path_read):\n",
    "    \n",
    "    all_path = (path_read + \"/\" + file_name)\n",
    "    img = cv2.imdecode(np.fromfile(all_path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    img_shape=img.shape\n",
    "    img_height=img_shape[0]\n",
    "    img_width=img_shape[1]\n",
    "   \n",
    "    os.makedirs(\"genki4k/files(preprocess)\", exist_ok=True)\n",
    "    path_save = \"genki4k/files(preprocess)\"\n",
    "\n",
    "    dets = detector(img , 1)\n",
    "    for k, d in enumerate(dets):\n",
    "        if len(dets) > 1:\n",
    "            continue\n",
    "        num=num+1\n",
    "        \n",
    "        pos_start = tuple([d.left(), d.top()])\n",
    "        pos_end = tuple([d.right(), d.bottom()])\n",
    " \n",
    "        height = d.bottom()-d.top()\n",
    "        width = d.right()-d.left()\n",
    " \n",
    "        img_blank = np.zeros((height, width, 3), np.uint8)\n",
    "        for i in range(height):\n",
    "            if d.top()+i >= img_height:  \n",
    "                continue\n",
    "            for j in range(width):\n",
    "                if d.left()+j >= img_width:  \n",
    "                    continue\n",
    "                img_blank[i][j] = img[d.top()+i][d.left()+j]\n",
    "        img_blank = cv2.resize(img_blank, (200, 200), interpolation=cv2.INTER_CUBIC)\n",
    " \n",
    "        cv2.imencode('.jpg', img_blank)[1].tofile(path_save+\"/\"+\"file\"+str(num)+\".jpg\") # æ­£ç¡®æ–¹æ³•\n",
    " \n",
    "print(\"processing done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa816e1-468a-43c6-9108-3f718ff8c632",
   "metadata": {},
   "source": [
    "8 Result \n",
    "Test the model on the 'test' dataset.The 'prob' is the probability that the given image belongs to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188bd256-8332-4af9-a7e2-5908bb083d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: genki4k/files(splitdata)/test/man_w\\file130.jpg  class: man_w  prob: 0.668\n",
      "image: genki4k/files(splitdata)/test/man_w\\file132.jpg  class: man_w  prob: 0.779\n",
      "image: genki4k/files(splitdata)/test/man_w\\file157.jpg  class: woman_w  prob: 0.503\n",
      "image: genki4k/files(splitdata)/test/man_w\\file16.jpg  class: man_w  prob: 0.993\n",
      "image: genki4k/files(splitdata)/test/man_w\\file186.jpg  class: man_w  prob: 0.999\n",
      "image: genki4k/files(splitdata)/test/man_w\\file194.jpg  class: man_w  prob: 0.955\n",
      "image: genki4k/files(splitdata)/test/man_w\\file204.jpg  class: woman_w  prob: 0.804\n",
      "image: genki4k/files(splitdata)/test/man_w\\file221.jpg  class: man_w  prob: 0.999\n",
      "image: genki4k/files(splitdata)/test/man_w\\file255.jpg  class: man_w  prob: 0.989\n",
      "image: genki4k/files(splitdata)/test/man_w\\file277.jpg  class: man_w  prob: 0.992\n",
      "image: genki4k/files(splitdata)/test/man_w\\file280.jpg  class: man_w  prob: 0.913\n",
      "image: genki4k/files(splitdata)/test/man_w\\file283.jpg  class: man_w  prob: 0.929\n",
      "image: genki4k/files(splitdata)/test/man_w\\file290.jpg  class: man_w  prob: 0.992\n",
      "image: genki4k/files(splitdata)/test/man_w\\file306.jpg  class: man_w  prob: 0.74\n",
      "image: genki4k/files(splitdata)/test/man_w\\file31.jpg  class: man_w  prob: 0.984\n",
      "image: genki4k/files(splitdata)/test/man_w\\file334.jpg  class: man_b  prob: 0.893\n",
      "image: genki4k/files(splitdata)/test/man_w\\file341.jpg  class: man_w  prob: 0.821\n",
      "image: genki4k/files(splitdata)/test/man_w\\file344.jpg  class: man_w  prob: 0.977\n",
      "image: genki4k/files(splitdata)/test/man_w\\file351.jpg  class: man_w  prob: 0.967\n",
      "image: genki4k/files(splitdata)/test/man_w\\file358.jpg  class: man_w  prob: 0.928\n",
      "image: genki4k/files(splitdata)/test/man_w\\file38.jpg  class: man_w  prob: 0.996\n",
      "image: genki4k/files(splitdata)/test/man_w\\file403.jpg  class: man_w  prob: 0.698\n",
      "image: genki4k/files(splitdata)/test/man_w\\file406.jpg  class: man_w  prob: 0.967\n",
      "image: genki4k/files(splitdata)/test/man_w\\file451.jpg  class: man_w  prob: 0.675\n",
      "image: genki4k/files(splitdata)/test/man_w\\file462.jpg  class: woman_w  prob: 0.72\n",
      "image: genki4k/files(splitdata)/test/man_w\\file487.jpg  class: man_w  prob: 0.921\n",
      "image: genki4k/files(splitdata)/test/man_w\\file488.jpg  class: man_w  prob: 0.977\n",
      "image: genki4k/files(splitdata)/test/man_w\\file490.jpg  class: man_w  prob: 0.983\n",
      "image: genki4k/files(splitdata)/test/man_w\\file495.jpg  class: man_w  prob: 0.981\n",
      "image: genki4k/files(splitdata)/test/man_w\\file498.jpg  class: man_w  prob: 0.998\n",
      "image: genki4k/files(splitdata)/test/man_w\\file500.jpg  class: man_w  prob: 0.987\n",
      "image: genki4k/files(splitdata)/test/man_w\\file506.jpg  class: man_w  prob: 0.888\n",
      "image: genki4k/files(splitdata)/test/man_w\\file507.jpg  class: man_w  prob: 0.963\n",
      "image: genki4k/files(splitdata)/test/man_w\\file510.jpg  class: woman_w  prob: 0.513\n",
      "image: genki4k/files(splitdata)/test/man_w\\file527.jpg  class: man_w  prob: 0.989\n",
      "image: genki4k/files(splitdata)/test/man_w\\file541.jpg  class: man_w  prob: 0.871\n",
      "image: genki4k/files(splitdata)/test/man_w\\file555.jpg  class: man_w  prob: 0.921\n",
      "image: genki4k/files(splitdata)/test/man_w\\file559.jpg  class: man_w  prob: 0.998\n",
      "image: genki4k/files(splitdata)/test/man_w\\file586.jpg  class: man_w  prob: 0.987\n",
      "image: genki4k/files(splitdata)/test/man_w\\file607.jpg  class: man_w  prob: 0.979\n",
      "image: genki4k/files(splitdata)/test/man_w\\file617.jpg  class: man_w  prob: 0.997\n",
      "image: genki4k/files(splitdata)/test/man_w\\file630.jpg  class: man_w  prob: 0.933\n",
      "image: genki4k/files(splitdata)/test/man_w\\file632.jpg  class: man_w  prob: 0.923\n",
      "image: genki4k/files(splitdata)/test/man_w\\file659.jpg  class: man_w  prob: 0.93\n",
      "image: genki4k/files(splitdata)/test/man_w\\file723.jpg  class: man_w  prob: 0.994\n",
      "image: genki4k/files(splitdata)/test/man_w\\file742.jpg  class: woman_w  prob: 0.906\n",
      "image: genki4k/files(splitdata)/test/man_w\\file79.jpg  class: man_w  prob: 0.834\n",
      "image: genki4k/files(splitdata)/test/man_w\\file87.jpg  class: woman_w  prob: 0.553\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file121.jpg  class: man_b  prob: 0.719\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file125.jpg  class: woman_w  prob: 0.928\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file128.jpg  class: man_w  prob: 0.951\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file259.jpg  class: woman_w  prob: 0.961\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file353.jpg  class: woman_w  prob: 0.964\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file354.jpg  class: woman_b  prob: 0.98\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file455.jpg  class: woman_b  prob: 0.476\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file522.jpg  class: woman_w  prob: 0.906\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file561.jpg  class: man_w  prob: 0.886\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file568.jpg  class: woman_b  prob: 0.967\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file585.jpg  class: woman_b  prob: 0.54\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file628.jpg  class: woman_w  prob: 0.9\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file682.jpg  class: woman_w  prob: 0.672\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file707.jpg  class: woman_w  prob: 0.984\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file729.jpg  class: man_w  prob: 0.749\n",
      "image: genki4k/files(splitdata)/test/woman_b\\file786.jpg  class: woman_b  prob: 0.943\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file103.jpg  class: woman_w  prob: 0.999\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file106.jpg  class: woman_w  prob: 0.988\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file115.jpg  class: woman_w  prob: 0.85\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file139.jpg  class: woman_w  prob: 0.413\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file141.jpg  class: woman_w  prob: 0.967\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file170.jpg  class: man_w  prob: 0.863\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file176.jpg  class: woman_w  prob: 0.991\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file181.jpg  class: man_w  prob: 0.934\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file188.jpg  class: woman_w  prob: 0.5\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file19.jpg  class: woman_w  prob: 0.582\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file205.jpg  class: woman_w  prob: 0.997\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file218.jpg  class: woman_y  prob: 0.906\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file229.jpg  class: man_w  prob: 0.64\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file234.jpg  class: man_w  prob: 0.459\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file237.jpg  class: man_w  prob: 0.977\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file242.jpg  class: woman_w  prob: 0.889\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file250.jpg  class: woman_w  prob: 0.989\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file252.jpg  class: woman_w  prob: 0.699\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file254.jpg  class: man_w  prob: 0.851\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file265.jpg  class: woman_w  prob: 0.983\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file266.jpg  class: woman_w  prob: 0.947\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file270.jpg  class: woman_w  prob: 0.813\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file321.jpg  class: woman_y  prob: 0.375\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file328.jpg  class: woman_w  prob: 0.972\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file335.jpg  class: woman_w  prob: 0.833\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file342.jpg  class: woman_w  prob: 0.992\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file345.jpg  class: woman_w  prob: 0.993\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file352.jpg  class: woman_w  prob: 0.815\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file356.jpg  class: woman_b  prob: 0.99\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file364.jpg  class: woman_w  prob: 0.948\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file366.jpg  class: woman_w  prob: 0.993\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file373.jpg  class: woman_w  prob: 0.516\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file374.jpg  class: woman_w  prob: 0.692\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file375.jpg  class: woman_w  prob: 0.839\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file379.jpg  class: woman_w  prob: 0.901\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file40.jpg  class: man_w  prob: 0.871\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file426.jpg  class: woman_w  prob: 0.846\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file430.jpg  class: woman_w  prob: 0.957\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file436.jpg  class: woman_w  prob: 0.447\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file443.jpg  class: woman_w  prob: 0.779\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file479.jpg  class: man_w  prob: 0.611\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file48.jpg  class: man_w  prob: 0.557\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file492.jpg  class: woman_w  prob: 0.652\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file519.jpg  class: woman_w  prob: 0.745\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file52.jpg  class: woman_w  prob: 0.984\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file521.jpg  class: woman_w  prob: 0.928\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file526.jpg  class: woman_w  prob: 0.903\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file539.jpg  class: woman_w  prob: 0.588\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file542.jpg  class: woman_y  prob: 0.447\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file543.jpg  class: woman_w  prob: 0.947\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file548.jpg  class: woman_w  prob: 0.898\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file55.jpg  class: woman_w  prob: 0.997\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file551.jpg  class: man_w  prob: 0.709\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file557.jpg  class: man_w  prob: 0.384\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file578.jpg  class: woman_w  prob: 0.831\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file616.jpg  class: woman_w  prob: 0.757\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file627.jpg  class: woman_w  prob: 0.61\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file641.jpg  class: woman_w  prob: 0.649\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file662.jpg  class: woman_w  prob: 0.975\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file677.jpg  class: woman_w  prob: 0.937\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file678.jpg  class: woman_w  prob: 0.982\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file688.jpg  class: woman_y  prob: 0.501\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file692.jpg  class: woman_w  prob: 0.987\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file702.jpg  class: woman_w  prob: 0.996\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file705.jpg  class: woman_w  prob: 0.986\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file714.jpg  class: woman_w  prob: 0.978\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file718.jpg  class: woman_w  prob: 0.988\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file725.jpg  class: woman_w  prob: 0.952\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file728.jpg  class: woman_w  prob: 0.919\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file734.jpg  class: woman_w  prob: 0.995\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file756.jpg  class: woman_w  prob: 0.497\n",
      "image: genki4k/files(splitdata)/test/woman_w\\file771.jpg  class: woman_w  prob: 0.994\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from resnet import resnet34\n",
    "\n",
    "\n",
    "def main(imgs_root):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    data_transform = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    # load image\n",
    "    # æŒ‡å‘éœ€è¦éåŽ†é¢„æµ‹çš„å›¾åƒæ–‡ä»¶å¤¹\n",
    "    # imgs_root = \"genki4k/files(splitdata)/test/unsmile\"\n",
    "    # imgs_root = \"genki4k/files(splitdata)/test/smile\"\n",
    "    assert os.path.exists(imgs_root), f\"file: '{imgs_root}' dose not exist.\"\n",
    "    # è¯»å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰jpgå›¾åƒè·¯å¾„\n",
    "    img_path_list = [os.path.join(imgs_root, i) for i in os.listdir(imgs_root) if i.endswith(\".jpg\")]\n",
    "\n",
    "    # read class_indict\n",
    "    json_path = './class_indices_advanced.json'\n",
    "    assert os.path.exists(json_path), f\"file: '{json_path}' dose not exist.\"\n",
    "\n",
    "    json_file = open(json_path, \"r\")\n",
    "    class_indict = json.load(json_file)\n",
    "\n",
    "    # create model\n",
    "    model = resnet34(num_classes=6).to(device)\n",
    "\n",
    "    # load model weights\n",
    "    weights_path = \"./resNet34_advanced.pth\"\n",
    "    assert os.path.exists(weights_path), f\"file: '{weights_path}' dose not exist.\"\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "\n",
    "    # prediction\n",
    "    model.eval()\n",
    "    batch_size = 8  # æ¯æ¬¡é¢„æµ‹æ—¶å°†å¤šå°‘å¼ å›¾ç‰‡æ‰“åŒ…æˆä¸€ä¸ªbatch\n",
    "    with torch.no_grad():\n",
    "        for ids in range(0, len(img_path_list) // batch_size):\n",
    "            img_list = []\n",
    "            for img_path in img_path_list[ids * batch_size: (ids + 1) * batch_size]:\n",
    "                assert os.path.exists(img_path), f\"file: '{img_path}' dose not exist.\"\n",
    "                img = Image.open(img_path)\n",
    "                img = data_transform(img)\n",
    "                img_list.append(img)\n",
    "\n",
    "            # batch img\n",
    "            # å°†img_liståˆ—è¡¨ä¸­çš„æ‰€æœ‰å›¾åƒæ‰“åŒ…æˆä¸€ä¸ªbatch\n",
    "            batch_img = torch.stack(img_list, dim=0)\n",
    "            # predict class\n",
    "            output = model(batch_img.to(device)).cpu()\n",
    "            predict = torch.softmax(output, dim=1)\n",
    "            probs, classes = torch.max(predict, dim=1)\n",
    "            for idx, (pro, cla) in enumerate(zip(probs, classes)):\n",
    "                print(\"image: {}  class: {}  prob: {:.3}\".format(img_path_list[ids * batch_size + idx],\n",
    "                                                                 class_indict[str(cla.numpy())],\n",
    "                                                                 pro.numpy()))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(\"genki4k/files(splitdata)/test/man_b\")\n",
    "    main(\"genki4k/files(splitdata)/test/man_w\")\n",
    "    main(\"genki4k/files(splitdata)/test/man_y\")\n",
    "    main(\"genki4k/files(splitdata)/test/woman_b\")\n",
    "    main(\"genki4k/files(splitdata)/test/woman_w\")\n",
    "    main(\"genki4k/files(splitdata)/test/woman_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf0d49-857d-4d4d-a5c7-be77808060bd",
   "metadata": {},
   "source": [
    "In this task, compared with basic part, we can easily see that the test accuracy is lower,easier to mis-classify.I think there are some reasons:\n",
    "1.The useful data is small, yellow woman and yellow man has relativly small data, about 120 images, More data for training will significantly improve accuracy.\n",
    "2.The distribution of data is not even, yellow people in the dataset are only a quarter of white people, it will affect prosterior probability, although we don't the exact prosterior probability. But obviously this affects the feature extraction.\n",
    "3.Each image is affected by light and shadow and pixels plus the fact that mixed races mix the characteristics of each race, requiring better predictions based on a larger and more comprehensive dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "06edd470-57ba-4b52-a98b-8ae4e950fa09",
   "metadata": {},
   "source": [
    "9 Conlusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8dbb90-1bc9-4094-aee8-752e01e2eaf2",
   "metadata": {},
   "source": [
    "A multi-classification task is quite similar to binary classification, labels are also very important in machine learning and nowdays it's an essential step.\n",
    "1. ResNet-34 exhibits powerful feature extraction capabilities, supported by its deep architecture and residual connections. This enables the network to learn more abstract and complex features, making it highly effective for tasks such as image classification.\n",
    "2.ResNet-34 is suitable for medium-sized and moderately complex image classification problems. For larger datasets or more intricate tasks, consideration of deeper networks may be necessary.\n",
    "3.ResNet-34 demonstrates good generalization ability across various image classification tasks. Due to its intermediate depth, it might perform well even on smaller datasets without significant risk of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
